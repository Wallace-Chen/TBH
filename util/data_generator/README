This folder is dedicated for preparing datasets as input to the DNNs

There are three datasets considered: CIFAR-10, NUS-WIDE, MS-COCO.

The general workflow is:
1. Prepare a .npz file (generated by np.savez_compressed) containing four arrays, corresponding to features_training, label_training, features_testing, label_testing. Label could be a vector or a scalar for each data sample. Features array in this point should be simply grey values of input images.
2. Feature extractor: given these four arrays in the first step, feed them into a pre-trained network to extract features. This step is finished by the feat_extractor.py under this folder. And another .npz file will be the output. Depending on the network chosen, the feature dimension could be 2048 or 512.
3. Convert data into TFRecords file that can be leveraged by the tensorflow networks. Do this step in the make_data.py file. Note that if in the first step, your label is a scalar, you should convert it into a vector in this file. The final .tfrecords files can be then placed under ./data folder, and start training your network!

Note:
1. For CIFAR-10 dataset, the first step above is skipped, since the KERAS has the API to simply get corresponding data. You can turn this function on in the feat_extractor.py file.
2. For NUS-WIDE and MS-COCO, please go to corresponding sub-folder to prepare the dataset for the step 1. Then comeback and continue with step 2 and 3.
